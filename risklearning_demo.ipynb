{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# risklearning demo\n",
    "\n",
    "Most, if not all, operational risk capital models assume the existence of stationary frequency and severity distributions (typically Poisson for frequencies, and a subexponential distribution such as lognormal for severities). Yet every quarter (or whenever the model is recalibrated) risk capital goes up almost without fail, either because frequencies increase, severities increase or both.\n",
    "\n",
    "The assumption of stationary distributions is just one limitation of current approaches to operational risk modeling, but it offers a good inroad for modeling approaches beyond the usual actuarial model typical in operational capital models.\n",
    "\n",
    "In this notebook, we give a first example of how neural networks can overcome the stationarity assumptions of traditional approaches. The hope is that this is but one of many examples showing a better way to model operational risk.\n",
    "\n",
    "Note: What follows if very much a work in progress . . .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import risklearning.learning_frequency as rlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up frequency distribution to generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tenors_horizon = 365 # (Time) tenors (e.g. 1 day) per model horizon (e.g. 1 year)\n",
    "\n",
    "h_start = 5.0 # How many model horizons of past data to train\n",
    "h_end = 1.0 #How many model horizons of past data to test / validate\n",
    "\n",
    "# Present is tenor 0, and boundary between training and testing data sets\n",
    "t_start = -int(math.floor(h_start*tenors_horizon))\n",
    "t_end = int(math.floor(h_end*tenors_horizon))\n",
    "\n",
    "\n",
    "#% Generate Poisson-distributed events\n",
    "lambda_init = 1 # intensity over tenor (e.g. day)\n",
    "lambda_final = 4 # intensity over tenor (e.g. day)\n",
    "n_tenors = t_end - t_start\n",
    "counts = rlf.sim_counts(lambda_init, lambda_final, n_tenors)\n",
    "\n",
    "# Build df around counts, level 1 and 2 categorization of Operational Risk events\n",
    "l1s = ['Execution Delivery and Process Management']*n_tenors\n",
    "l2s = ['Transaction Capture, Execution and Maintenance']*n_tenors\n",
    "tenors = list(xrange(t_start, t_end))\n",
    "\n",
    "counts_sim_df = pd.DataFrame({'t': tenors,\n",
    "                              'OR Category L1': l1s, 'OR Category L2': l2s,\n",
    "                              'counts': counts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "                    \n",
    "#%% Do MLE (simple average for Poisson process\n",
    "n_tenors_train = -t_start\n",
    "n_tenors_test = t_end\n",
    "\n",
    "counts_train = (counts_sim_df[counts_sim_df.t < 0]).groupby('OR Category L2').sum()\n",
    "counts_test =  (counts_sim_df[counts_sim_df.t >= 0]).groupby('OR Category L2').sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE for training data\n",
    "\n",
    "For the Poisson distribution, the MLE of the intensity (here lambda) is just the average of the counts per model horizon. In practice, OpRisk models sometimes take a weighted average, with the weight linearly decreasing over a period of years (see e.g. \"LDA at Work\" by Aue and Kalkbrener)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>9</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.107102</td>\n",
       "      <td>0.239263</td>\n",
       "      <td>0.267254</td>\n",
       "      <td>0.199012</td>\n",
       "      <td>0.111147</td>\n",
       "      <td>0.04966</td>\n",
       "      <td>0.026447</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4        5         9    \\\n",
       "0  0.107102  0.239263  0.267254  0.199012  0.111147  0.04966  0.026447   \n",
       "\n",
       "        100  \n",
       "0  0.000114  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lambdas_train = counts_train['counts']/n_tenors_train\n",
    "lambdas_test = counts_train['counts']/n_tenors_test\n",
    "\n",
    "bin_tops = [1,2,3,4,5,6,10,101]\n",
    "# Recall that digitize (used later) defines bins by lower <= x < upper\n",
    "count_tops =[count - 1 for count in bin_tops]\n",
    "\n",
    "# Calculate bin probabilities from MLE poisson\n",
    "poi_mle = stats.poisson(lambdas_train)\n",
    "poi_tops = poi_mle.cdf(count_tops)\n",
    "poi_tops_shifted = poi_mle.cdf(count_tops[1:])\n",
    "poi_bins = np.insert(poi_tops_shifted - poi_tops[:-1], 0, poi_tops[0])\n",
    "\n",
    "#mle_probs = pd.DataFrame({'Count Top': [t-1 for t in bin_tops], 'Probs': poi_bins})\n",
    "mle_probs = pd.DataFrame(poi_bins, index = [t-1 for t in bin_tops])\n",
    "mle_probs.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep simulated losses for neural network\n",
    "\n",
    "For example\n",
    "\n",
    "* Use one-hot-encoding for L1 and L2 categories (this will make more sense once we look at multiple dependent categories)\n",
    "* Bin count data\n",
    "* Normalize tenors (i.e. scale so that first tenor maps to -1 with 0 preserved)\n",
    "* Export as numpy arrays to feed into keras / tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # TODO: improve slicing to avoid warnings\n",
    "\n",
    "x_train, y_train, x_test, y_test = rlf.prep_count_data(counts_sim_df, bin_tops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the network architecture and train\n",
    "\n",
    "We use keras with TensorFlow backend.\n",
    "\n",
    "Note: there has been no real attempt yet to optimize metaparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1825 samples, validate on 364 samples\n",
      "Epoch 1/4\n",
      "1825/1825 [==============================] - 0s - loss: 1.8859 - val_loss: 2.0922\n",
      "Epoch 2/4\n",
      "1825/1825 [==============================] - 0s - loss: 1.8086 - val_loss: 2.0928\n",
      "Epoch 3/4\n",
      "1825/1825 [==============================] - 0s - loss: 1.7974 - val_loss: 2.0442\n",
      "Epoch 4/4\n",
      "1825/1825 [==============================] - 0s - loss: 1.7682 - val_loss: 2.0188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0c294a7790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "hlayer_len = [100] # As series in anticipation of different sized layers\n",
    "\n",
    "# Number of nodes in output layer: if series, 1, else number of cols\n",
    "out_layer_len = 1 if len(y_train.shape)==1 else y_train.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(hlayer_len[0], input_shape=(x_train.shape[1],)))\n",
    "model.add(Activation('relu')) # An \"activation\" is just a non-linear function applied to the output\n",
    "                              # of the layer above. Here, with a \"rectified linear unit\",\n",
    "                              # we clamp all values below 0 to 0.\n",
    "                           \n",
    "model.add(Dropout(0.2))   # Default dropout parameter\n",
    "model.add(Dense(hlayer_len[0]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(hlayer_len[0]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(out_layer_len))\n",
    "model.add(Activation('softmax')) \n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# For categorical target\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32, nb_epoch=4,\n",
    "          show_accuracy=True, verbose=1,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network frequency distribution\n",
    "\n",
    "If the neural network has learned anything, we will see that the probility distribution shifts over time to higher buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/364 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.11700331,  0.1759285 ,  0.21116692, ...,  0.08428088,\n",
       "         0.05115734,  0.009389  ],\n",
       "       [ 0.11697587,  0.17590062,  0.21113475, ...,  0.08431186,\n",
       "         0.05117796,  0.00939477],\n",
       "       [ 0.1169484 ,  0.17587277,  0.21110259, ...,  0.08434284,\n",
       "         0.05119858,  0.00940054],\n",
       "       ..., \n",
       "       [ 0.11159051,  0.17003162,  0.20318198, ...,  0.09117258,\n",
       "         0.05655767,  0.01094004],\n",
       "       [ 0.11158334,  0.17002377,  0.20316948, ...,  0.09118284,\n",
       "         0.05656745,  0.01094274],\n",
       "       [ 0.11157613,  0.17001592,  0.20315693, ...,  0.09119311,\n",
       "         0.05657726,  0.01094543]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = model.predict_proba(x_test, batch_size=32)\n",
    "proba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>9</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.117003</td>\n",
       "      <td>0.175929</td>\n",
       "      <td>0.211167</td>\n",
       "      <td>0.212467</td>\n",
       "      <td>0.138607</td>\n",
       "      <td>0.084281</td>\n",
       "      <td>0.051157</td>\n",
       "      <td>0.009389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.116976</td>\n",
       "      <td>0.175901</td>\n",
       "      <td>0.211135</td>\n",
       "      <td>0.212472</td>\n",
       "      <td>0.138633</td>\n",
       "      <td>0.084312</td>\n",
       "      <td>0.051178</td>\n",
       "      <td>0.009395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.116948</td>\n",
       "      <td>0.175873</td>\n",
       "      <td>0.211103</td>\n",
       "      <td>0.212476</td>\n",
       "      <td>0.138658</td>\n",
       "      <td>0.084343</td>\n",
       "      <td>0.051199</td>\n",
       "      <td>0.009401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.116921</td>\n",
       "      <td>0.175845</td>\n",
       "      <td>0.211070</td>\n",
       "      <td>0.212480</td>\n",
       "      <td>0.138684</td>\n",
       "      <td>0.084374</td>\n",
       "      <td>0.051219</td>\n",
       "      <td>0.009406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.116894</td>\n",
       "      <td>0.175817</td>\n",
       "      <td>0.211038</td>\n",
       "      <td>0.212485</td>\n",
       "      <td>0.138709</td>\n",
       "      <td>0.084405</td>\n",
       "      <td>0.051240</td>\n",
       "      <td>0.009412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         9    \\\n",
       "0  0.117003  0.175929  0.211167  0.212467  0.138607  0.084281  0.051157   \n",
       "1  0.116976  0.175901  0.211135  0.212472  0.138633  0.084312  0.051178   \n",
       "2  0.116948  0.175873  0.211103  0.212476  0.138658  0.084343  0.051199   \n",
       "3  0.116921  0.175845  0.211070  0.212480  0.138684  0.084374  0.051219   \n",
       "4  0.116894  0.175817  0.211038  0.212485  0.138709  0.084405  0.051240   \n",
       "\n",
       "        100  \n",
       "0  0.009389  \n",
       "1  0.009395  \n",
       "2  0.009401  \n",
       "3  0.009406  \n",
       "4  0.009412  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_probs = pd.DataFrame(proba, index = range(0,t_end-1), columns = [t-1 for t in bin_tops])\n",
    "# Heads (i.e. starting from present)\n",
    "nn_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>9</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.111605</td>\n",
       "      <td>0.170047</td>\n",
       "      <td>0.203207</td>\n",
       "      <td>0.210613</td>\n",
       "      <td>0.145903</td>\n",
       "      <td>0.091152</td>\n",
       "      <td>0.056538</td>\n",
       "      <td>0.010935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.111598</td>\n",
       "      <td>0.170039</td>\n",
       "      <td>0.203195</td>\n",
       "      <td>0.210604</td>\n",
       "      <td>0.145917</td>\n",
       "      <td>0.091162</td>\n",
       "      <td>0.056548</td>\n",
       "      <td>0.010937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.111591</td>\n",
       "      <td>0.170032</td>\n",
       "      <td>0.203182</td>\n",
       "      <td>0.210595</td>\n",
       "      <td>0.145930</td>\n",
       "      <td>0.091173</td>\n",
       "      <td>0.056558</td>\n",
       "      <td>0.010940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.111583</td>\n",
       "      <td>0.170024</td>\n",
       "      <td>0.203169</td>\n",
       "      <td>0.210586</td>\n",
       "      <td>0.145944</td>\n",
       "      <td>0.091183</td>\n",
       "      <td>0.056567</td>\n",
       "      <td>0.010943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.111576</td>\n",
       "      <td>0.170016</td>\n",
       "      <td>0.203157</td>\n",
       "      <td>0.210577</td>\n",
       "      <td>0.145958</td>\n",
       "      <td>0.091193</td>\n",
       "      <td>0.056577</td>\n",
       "      <td>0.010945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         9    \\\n",
       "359  0.111605  0.170047  0.203207  0.210613  0.145903  0.091152  0.056538   \n",
       "360  0.111598  0.170039  0.203195  0.210604  0.145917  0.091162  0.056548   \n",
       "361  0.111591  0.170032  0.203182  0.210595  0.145930  0.091173  0.056558   \n",
       "362  0.111583  0.170024  0.203169  0.210586  0.145944  0.091183  0.056567   \n",
       "363  0.111576  0.170016  0.203157  0.210577  0.145958  0.091193  0.056577   \n",
       "\n",
       "          100  \n",
       "359  0.010935  \n",
       "360  0.010937  \n",
       "361  0.010940  \n",
       "362  0.010943  \n",
       "363  0.010945  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tails (i.e. going to end of model horizon of 1 yr)\n",
    "nn_probs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>9</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.107102</td>\n",
       "      <td>0.239263</td>\n",
       "      <td>0.267254</td>\n",
       "      <td>0.199012</td>\n",
       "      <td>0.111147</td>\n",
       "      <td>0.04966</td>\n",
       "      <td>0.026447</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4        5         9    \\\n",
       "0  0.107102  0.239263  0.267254  0.199012  0.111147  0.04966  0.026447   \n",
       "\n",
       "        100  \n",
       "0  0.000114  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And what MLE told us before\n",
    "mle_probs.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and next steps\n",
    "\n",
    "We can see by the nn_probs data frame that the probability mass of the neural network shifts to the right, as does the underlying Poisson processes, with its intensity starting at 1 events per tenor / day at - 5 yrs and ending at 4 events per tenor / day at +1 yrs.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "* Use better metric on generalization error that looking at probability tables (KS?)\n",
    "* Optimize hyperparameters\n",
    "* Simulate multiple, correlated Poisson processes\n",
    "* Test non-linear non-stationarities\n",
    "* Try recurrent neural network\n",
    "* Try convolution network\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [env_rl]",
   "language": "python",
   "name": "Python [env_rl]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
